{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131fb19bd55941b19ddceb405ec268e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d78c81bb8d4f30b36613ecef2227f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a67635d51564927876a53834a4c7000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149140e1b4454dd9b1c226127e051183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:\\Users\\thejo\\Documents\\school\\CSCI8960 Privacy Preserving Data Analysis\\termProject\\FMNISTData\\FashionMNIST\\raw\n",
      "Processing...\n",
      "Done!\n",
      "Epoch [1/6], Step [100/600], Loss: 0.4914, Accuracy: 78.00%\n",
      "Epoch [1/6], Step [200/600], Loss: 0.4007, Accuracy: 83.00%\n",
      "Epoch [1/6], Step [300/600], Loss: 0.4831, Accuracy: 78.00%\n",
      "Epoch [1/6], Step [400/600], Loss: 0.3603, Accuracy: 85.00%\n",
      "Epoch [1/6], Step [500/600], Loss: 0.2768, Accuracy: 88.00%\n",
      "Epoch [1/6], Step [600/600], Loss: 0.3332, Accuracy: 88.00%\n",
      "Epoch [2/6], Step [100/600], Loss: 0.4045, Accuracy: 86.00%\n",
      "Epoch [2/6], Step [200/600], Loss: 0.3151, Accuracy: 90.00%\n",
      "Epoch [2/6], Step [300/600], Loss: 0.2655, Accuracy: 91.00%\n",
      "Epoch [2/6], Step [400/600], Loss: 0.3937, Accuracy: 89.00%\n",
      "Epoch [2/6], Step [500/600], Loss: 0.2930, Accuracy: 88.00%\n",
      "Epoch [2/6], Step [600/600], Loss: 0.1981, Accuracy: 93.00%\n",
      "Epoch [3/6], Step [100/600], Loss: 0.4487, Accuracy: 84.00%\n",
      "Epoch [3/6], Step [200/600], Loss: 0.2865, Accuracy: 91.00%\n",
      "Epoch [3/6], Step [300/600], Loss: 0.3853, Accuracy: 87.00%\n",
      "Epoch [3/6], Step [400/600], Loss: 0.2406, Accuracy: 90.00%\n",
      "Epoch [3/6], Step [500/600], Loss: 0.2692, Accuracy: 89.00%\n",
      "Epoch [3/6], Step [600/600], Loss: 0.3209, Accuracy: 91.00%\n",
      "Epoch [4/6], Step [100/600], Loss: 0.2598, Accuracy: 90.00%\n",
      "Epoch [4/6], Step [200/600], Loss: 0.2326, Accuracy: 91.00%\n",
      "Epoch [4/6], Step [300/600], Loss: 0.3268, Accuracy: 91.00%\n",
      "Epoch [4/6], Step [400/600], Loss: 0.2916, Accuracy: 91.00%\n",
      "Epoch [4/6], Step [500/600], Loss: 0.2692, Accuracy: 84.00%\n",
      "Epoch [4/6], Step [600/600], Loss: 0.1609, Accuracy: 97.00%\n",
      "Epoch [5/6], Step [100/600], Loss: 0.4370, Accuracy: 86.00%\n",
      "Epoch [5/6], Step [200/600], Loss: 0.3567, Accuracy: 89.00%\n",
      "Epoch [5/6], Step [300/600], Loss: 0.2611, Accuracy: 93.00%\n",
      "Epoch [5/6], Step [400/600], Loss: 0.2406, Accuracy: 90.00%\n",
      "Epoch [5/6], Step [500/600], Loss: 0.3052, Accuracy: 91.00%\n",
      "Epoch [5/6], Step [600/600], Loss: 0.2987, Accuracy: 88.00%\n",
      "Epoch [6/6], Step [100/600], Loss: 0.1881, Accuracy: 94.00%\n",
      "Epoch [6/6], Step [200/600], Loss: 0.3232, Accuracy: 90.00%\n",
      "Epoch [6/6], Step [300/600], Loss: 0.3552, Accuracy: 87.00%\n",
      "Epoch [6/6], Step [400/600], Loss: 0.2851, Accuracy: 89.00%\n",
      "Epoch [6/6], Step [500/600], Loss: 0.1775, Accuracy: 94.00%\n",
      "Epoch [6/6], Step [600/600], Loss: 0.3193, Accuracy: 89.00%\n",
      "Test Accuracy of the model on the 10000 test images: 90.42999999999999 %\n"
     ]
    }
   ],
   "source": [
    "# Jonathan Vance\n",
    "# May 1, 2020\n",
    "# this code is mostly copied from:\n",
    "# https://github.com/adventuresinML/adventures-in-ml-code/blob/master/conv_net_py_torch.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 6\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "DATA_PATH = 'C:\\\\Users\\\\thejo\\\\Documents\\\\school\\\\CSCI8960 Privacy Preserving Data Analysis\\\\termProject\\\\FMNISTData'\n",
    "MODEL_STORE_PATH = 'C:\\\\Users\\\\thejo\\\\Documents\\\\school\\\\CSCI8960 Privacy Preserving Data Analysis\\\\termProject\\\\pytorch_models\\\\'\n",
    "\n",
    "# transforms to apply to the data\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root=DATA_PATH, train=True, transform=trans, download=True)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root=DATA_PATH, train=False, transform=trans)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
    "\n",
    "# Save the model and plot\n",
    "torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')\n",
    "\n",
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='PyTorch ConvNet results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
